# -*- coding: utf-8 -*-
"""Flipkart  Product Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11K7lCvhqbTqW4wspROw9POKb8NJtae1j
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS

from google.colab import drive
drive.mount('/content/drive')

flipkart_data = pd.read_csv('/content/drive/MyDrive/Dataset-SA.csv')

flipkart_data

flipkart_data.info()

flipkart_data.drop(columns=['product_name'], inplace=True)
flipkart_data.drop(columns=['product_price'], inplace=True)
flipkart_data.drop(columns=['Rate'], inplace=True)
flipkart_data.drop(columns=['Review'], inplace=True)

flipkart_data.isnull().sum()

flipkart_data = flipkart_data.dropna()

#Libraries of NLTK
import re,string,nltk
import tensorflow as tf
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import wordnet
from nltk.stem import PorterStemmer,WordNetLemmatizer

#cleaning the data from unnecessary data

#replacing url with string URL
def replace_url(text):
    return re.sub('https?:\/\/\S*|www\.\S+','URL',text)

#removing html
def remove_html(text):
    return re.sub('<.*?>','',text)

#replacing mentions with string user
def replace_mentions(text):
    return re.sub('@\S*','user',text,flags=re.IGNORECASE)

#replacing numbers with string number
def replace_num(text):
    return re.sub('^[+-]*?\d{1,3}[- ]*?\d{1,10}|\d{10}','NUMBER',text)

#replacing <3 with sring heart
def replace_heart(text):
    return re.sub('<3','HEART', text)

#removing alphanumeric characters eg-XYZ123ABC
def remove_alphanumeric(text):
    return re.sub('\w*\d+\w*','',text)

#removing all english stopwords
def remove_stopwords(text):
    text = ' '.join([word for word in text.split() if word not in stopwords.words("english")])
    return text

#removing punctuations
def remove_punctuations(text):
    text=''.join([word for word in text if word not in string.punctuation])
    return text

#reducing words to thier root form
def lemmatization(text):
    lm= WordNetLemmatizer()
    text = ' '.join([lm.lemmatize(word, pos='v') for word in text.split()])
    return text

def clean_text(text):
    text=str(text).lower()
    text = replace_url(text)
    text = remove_html(text)
    text = replace_mentions(text)
    text = replace_num(text)
    text = replace_heart(text)
    text = remove_alphanumeric(text)
    text = remove_stopwords(text)
    text=remove_punctuations(text)
    #text=stemming(text)
    text=lemmatization(text)
    return text

flipkart_data['Summary']=flipkart_data['Summary'].apply(clean_text)

# positive -> 1
# negative -> 0
# Mapping dictionary
sentiment_mapping = {"positive": 1, "negative": 0, "neutral": 1}

# Replace values based on the mapping
flipkart_data['Sentiment'] = flipkart_data['Sentiment'].replace(sentiment_mapping)

flipkart_data["Sentiment"].value_counts()

flipkart_data.head()

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

train_data, test_data = train_test_split(flipkart_data, test_size = 0.2, random_state=42)

train_data.shape

test_data.shape

tokenizer = Tokenizer(num_words = 3500)
tokenizer.fit_on_texts(train_data["Summary"])

X_train = pad_sequences(tokenizer.texts_to_sequences(train_data["Summary"]), maxlen=50)
X_test = pad_sequences(tokenizer.texts_to_sequences(test_data["Summary"]), maxlen=50)
Y_train = train_data["Sentiment"]
Y_test = test_data["Sentiment"]

model = Sequential()
model.add(Embedding(input_dim =3500, output_dim = 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout = 0.2))
model.add(Dense(1, activation = "sigmoid"))

model.summary()

model.compile(optimizer = "adam", loss="binary_crossentropy", metrics=["accuracy"])

model.fit(X_train, Y_train, epochs = 10, batch_size = 64, validation_split = 0.2)

model.fit(X_train, Y_train, epochs = 5, batch_size = 64, validation_split = 0.2)

model.fit(X_train, Y_train, epochs = 5, batch_size = 64, validation_split = 0.2)

model.save("flip_model.h5")

import joblib
joblib.dump(tokenizer, "flip_tokenizer.pkl")

loss, accuracy = model.evaluate(X_test, Y_test)

def predictive_system(review):
  sequences = tokenizer.texts_to_sequences([review])
  padded_sequence = pad_sequences(sequences, maxlen=50)
  prediction = model.predict(padded_sequence)
  sentiment = "positive" if prediction[0][0] > 0.5 else "negative"
  return sentiment

predictive_system("This product is bad")

from keras.models import load_model
import joblib
from tensorflow.keras.preprocessing.sequence import pad_sequences

model = load_model("/content/flip_model.h5")
tokenizer = joblib.load("/content/flip_tokenizer.pkl")

def predictive_system(review):
  sequences = tokenizer.texts_to_sequences([review])
  padded_sequence = pad_sequences(sequences, maxlen=50)
  prediction = model.predict(padded_sequence)
  sentiment = "positive" if prediction[0][0] > 0.5 else "negative"
  return sentiment

pip install gradio

import gradio as gr
title = "FLIPKART PRODUCT SENTIMENT ANALYSIS APPLICATION"

app = gr.Interface(fn = predictive_system, inputs="textbox", outputs="textbox", title=title)

app.launch(share=True)